{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "presentation_notebook.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CByI5WZ1T6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "#Sentiment Analysis of Online Amazon Reviews\n",
        "###################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZiRAHNYWXDG",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Analysis of Online Amazon Reviews\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmKAdGhOC94p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2ce932a2-81e6-4798-913b-2fbb1d1c8c05"
      },
      "source": [
        "#import dependencies\n",
        "!pip install pyspark\n",
        "from pyspark import SparkFiles\n",
        "from pyspark import SparkContext\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n",
            "\u001b[K     |████████████████████████████████| 215.6MB 119kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 35.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehFMPqyyVzOn",
        "colab_type": "text"
      },
      "source": [
        "**PySpark is compatible with Java 8, not Java 11. Below, we search and install an open-source version of Java 8. We then set Java 8 as the default java program for this notebook.**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0e4lsT8DC6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4f6faf9e-afed-41ae-d1e7-91146fd29bb8"
      },
      "source": [
        "# ! sudo apt search openjdk\n",
        "# ! apt-get install openjdk-8-jdk-headless\n",
        "# ! echo 2 | sudo update-alternatives --config javac\n",
        "# ! sudo update-java-alternatives --set /usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
        "\n",
        "! sudo update-alternatives --config javac \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is only one alternative in link group javac (providing /usr/bin/javac): /usr/lib/jvm/java-8-openjdk-amd64/bin/javac\n",
            "Nothing to configure.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3SbNB0uEOSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the training data from AWS, using PySpark\n",
        "\n",
        "url = \"https://s3-us-west-1.amazonaws.com/emansbucket/SentimentAnalysis/train.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"train.csv\"), inferSchema=True, sep=',')\n",
        "df.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmuCB8RSGDQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the words\n",
        "tokenizer = Tokenizer(inputCol=\"comments\", outputCol=\"token_words\")\n",
        "# tokenizer\n",
        "# tokenizer id _fb45f637d262"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iExIopIWGDfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform and display DF with tokenizer output col. \n",
        "# Tokenizing means separating each word by commas within a list\n",
        "\n",
        "# Transform and show DataFrame\n",
        "tokenized = tokenizer.transform(df)\n",
        "tokenized.show(truncate=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbxVjH8-GMCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform stop-words filtering\n",
        "\n",
        "# Instantiate Remover\n",
        "remover = StopWordsRemover(inputCol=\"token_words\", outputCol=\"filtered\")\n",
        "# Transform and show data\n",
        "remover.transform(tokenized).show(truncate=True)\n",
        "\n",
        "\n",
        "tk_stpwrds_df = remover.transform(tokenized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxEbmvDpGMLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform NLP hashing\n",
        "\n",
        "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,8))\n",
        "\n",
        "# Transform into a spark DF\n",
        "hashed_df = hashing.transform(tk_stpwrds_df)\n",
        "#display new DF\n",
        "hashed_df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwJK2hWkGMO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Term freq- inverse document frequency.\n",
        "# TF--count of each word in a document.\n",
        "# IDF-- the more prevalent the word, the lower the IDF score\n",
        "\n",
        "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel = idf.fit(hashed_df)\n",
        "rescaledData = idfModel.transform(hashed_df)\n",
        "\n",
        "# Display the DataFrame w/ original 'comments' and 'features' col\n",
        "feat_df = rescaledData.select([\"comments\",\"token_words\", \"filtered\", \"hashedValues\", \"features\"])\n",
        "\n",
        "# Note, .show() will not return new DF . Only select the col\n",
        "\n",
        "feat_df.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etjRQPc7GMTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function on tokens using square brackets slicing method.\n",
        "# Separate the comment label ('1' or '2') from the comment string itself\n",
        "\n",
        "def array_chop(arry):\n",
        "    fstring = arry[0]\n",
        "    digit = fstring.replace(chr(34),'')\n",
        "    # Note, in binary chr(34) is \"\n",
        "    \n",
        "    return int(digit)\n",
        "\n",
        "# Create a user defined function \n",
        "chop = udf(array_chop, IntegerType())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWjv7iZiIdE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select the necessary columns, and do not truncate results\n",
        "# One MUST label the output column 'label' to use Naive Bayes Model\n",
        "\n",
        "final_df = feat_df.select(\"token_words\", \"features\").withColumn(\"label\", chop(col(\"token_words\")))\n",
        "final_df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZAAVQIOVCzf",
        "colab_type": "text"
      },
      "source": [
        "# Create a Naive Bayes model and fit the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJImUsPMIeTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the Naive Bayes classifier\n",
        "# Note, the final spark DataFrame MUST have a 'label' and 'features' column \n",
        "nb = NaiveBayes()\n",
        "# Fit the model onto our dataset\n",
        "predictor = nb.fit(final_df)\n",
        "\n",
        "# Now that we have trained our model, we can transform the model with our testing data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myFc4QdhUr6T",
        "colab_type": "text"
      },
      "source": [
        "# Read in and pre-process the testing data. Then transform the model with it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LleLxMpnIeaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url1 = \"https://s3-us-west-1.amazonaws.com/emansbucket/SentimentAnalysis/test.csv\"\n",
        "#sc = SparkContext('local')\n",
        "spark = SparkSession(sc)\n",
        "spark.sparkContext.addFile(url1)\n",
        "\n",
        "df1 = spark.read.option(\"header\", \"true\").csv(SparkFiles.get(\"test.csv\"), inferSchema=True, sep=',')\n",
        "df1.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv92KKvEGMV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizer the words\n",
        "tokenizer1 = Tokenizer(inputCol=\"comments\", outputCol=\"token_words\")\n",
        "tokenizer1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2mn3fSSK6is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform and display DF with tokenizer output column\n",
        "tokenized1 = tokenizer1.transform(df1)\n",
        "tokenized1.show(truncate=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIDir1NlK6tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform stop words filtering\n",
        "\n",
        "# Instantiate Remover\n",
        "remover1 = StopWordsRemover(inputCol=\"token_words\", outputCol=\"filtered\")\n",
        "# Transform and show data\n",
        "remover1.transform(tokenized1).show(truncate=True)\n",
        "tk_stpwrds_df1 = remover1.transform(tokenized1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Gpp_XWK62N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform NLP hashing\n",
        "\n",
        "hashing1 = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,8))\n",
        "\n",
        "# Transform into a DF\n",
        "hashed_df1 = hashing1.transform(tk_stpwrds_df1)\n",
        "#display new DF\n",
        "hashed_df1.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_D941aILbnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Term freq- inverse document frequency.\n",
        "idf1 = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel1 = idf1.fit(hashed_df1)\n",
        "rescaledData1 = idfModel1.transform(hashed_df1)\n",
        "\n",
        "# Display the DataFrame w/ original 'comments' col and 'features' col\n",
        "feat_df1 = rescaledData1.select([\"comments\",\"token_words\", \"filtered\", \"hashedValues\", \"features\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqKSa-7cLbqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate the comment label ('1' or '2') from the comment string itself using chop function\n",
        "final_df1 = feat_df1.select(\"token_words\", \"features\").withColumn(\"label\", chop(col(\"token_words\")))\n",
        "final_df1.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuKoYQJuUTol",
        "colab_type": "text"
      },
      "source": [
        "# We have now pre-processed all of our data. \n",
        "# Below, we can transform the model with our testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWB6mj90MYkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tranform the model with the testing data\n",
        "test_results = predictor.transform(final_df1)\n",
        "test_results.show(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m0QMeL4MYmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating our model's accuracy \n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_gR96RDTlh4",
        "colab_type": "text"
      },
      "source": [
        "# Transform user input into the format necessary for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qhLNWjHM40y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_string = \".....................................\"\n",
        "raw_data = {'comments': [input_string]}\n",
        "\n",
        "input_df = pd.DataFrame(raw_data, columns=['comments'])\n",
        "input_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrpVlcqQM45J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From google.colab import files\n",
        "\n",
        "input_df.to_csv(\"user_input1.csv\")\n",
        "#files.download(\"user_input.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXuup5qdM48o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"user_input1.csv\"\n",
        "#path = \"/Users/emmanuelfabre/Desktop/sentiment_analysis/user_input.csv\"\n",
        "spark = SparkSession(sc)\n",
        "spark.sparkContext.addFile(path)\n",
        "spdf = spark.read.option('header', 'true').csv(SparkFiles.get(\"user_input1.csv\"), inferSchema=True, sep=',')\n",
        "spdf.show(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwTlfSofM5BJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "372eeab7-0b91-4014-d9d6-423b46859664"
      },
      "source": [
        "# Tokenize user input\n",
        "tokenizer2 = Tokenizer(inputCol=\"comments\", outputCol=\"token_words\")\n",
        "tokenizer2\n",
        "\n",
        "# Transform and show DataFrame\n",
        "tokenized2 = tokenizer2.transform(spdf)\n",
        "tokenized2.show(truncate=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b4618b69e1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"comments\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"token_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/feature.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputCol, outputCol)\u001b[0m\n\u001b[1;32m   2717\u001b[0m         \"\"\"\n\u001b[1;32m   2718\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2719\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.spark.ml.feature.Tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2720\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m     62\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36m_jvm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Cannot load _jvm from SparkContext. Is SparkContext initialized?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91-wSmLpNQTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform Stop Words Removal \n",
        "\n",
        "# Instantiate Remover\n",
        "remover2 = StopWordsRemover(inputCol=\"token_words\", outputCol=\"filtered\")\n",
        "# Transform and show data\n",
        "remover2.transform(tokenized2).show(truncate=True)\n",
        "\n",
        "tk_stpwrds_df2 = remover2.transform(tokenized2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XelXudpYNdVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform NLP hashing\n",
        "hashing2 = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,8))\n",
        "\n",
        "# Transform into a DF\n",
        "hashed_df2 = hashing2.transform(tk_stpwrds_df2)\n",
        "# Display new DF\n",
        "hashed_df2.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgl73-omSgL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Term freq- inverse document frequency.\n",
        "\n",
        "idf2 = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel2 = idf2.fit(hashed_df2)\n",
        "rescaledData2 = idfModel2.transform(hashed_df2)\n",
        "\n",
        "feat_df2 = rescaledData2.select([\"comments\",\"token_words\", \"filtered\", \"hashedValues\", \"features\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjURQutpNdYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate the comment label from the comment using chop function\n",
        "\n",
        "final_df2 = feat_df2.select(\"token_words\", \"features\").withColumn(\"label\", chop(col(\"token_words\")))\n",
        "#final_df2 = feat_df2.select(\"token_words\", \"features\").withColumn(\"label\", \"1\")\n",
        "final_df2.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E5ztcCvNda_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the model onto the user input DF\n",
        "input_results = predictor.transform(final_df2)\n",
        "input_results.show(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-rQnj7NddZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}